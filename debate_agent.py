"""
í† ë¡  ì—ì´ì „íŠ¸ êµ¬í˜„ - GitHub ì €ì¥ì†Œë“¤ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ í†µí•©
"""

from typing import List, Dict, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
import json
import logging
import asyncio
import os

class AgentRole(Enum):
    """Agent4Debateì˜ ì—­í•  ê¸°ë°˜ ì ‘ê·¼ë²•"""
    SEARCHER = "searcher"  # ì •ë³´ ê²€ìƒ‰ ë‹´ë‹¹
    ANALYZER = "analyzer"  # ë…¼ì¦ ë¶„ì„ ë‹´ë‹¹
    WRITER = "writer"      # ë…¼ì¦ ìƒì„± ë‹´ë‹¹
    REVIEWER = "reviewer"  # í’ˆì§ˆ ê²€í†  ë‹´ë‹¹
    DEVIL = "devil"        # MADì˜ ë°˜ëŒ€ ì…ì¥
    ANGEL = "angel"        # MADì˜ ì§€ì§€ ì…ì¥
    ORGANIZER = "organizer"  # í† ë¡  ì§„í–‰ì (ìƒˆë¡œ ì¶”ê°€)

class DebateStance(Enum):
    """í† ë¡  ì…ì¥"""
    SUPPORT = "support"
    OPPOSE = "oppose"
    NEUTRAL = "neutral"

@dataclass
class Argument:
    """ë…¼ì¦ ë°ì´í„° êµ¬ì¡° (KITECH ë°©ì‹ í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€)"""
    content: str
    agent_name: str
    stance: DebateStance
    round_number: int
    evidence: List[str] = None
    confidence_score: float = 0.0
    quality_score: float = 0.7  # KITECH ë°©ì‹ í’ˆì§ˆ ì ìˆ˜
    
class DebateAgent:
    """í†µí•© í† ë¡  ì—ì´ì „íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        name: str,
        role: AgentRole,
        stance: DebateStance,
        model: str = "llama3.2:3b",
        persona_prompt: str = None,
        temperature: float = 0.7
    ):
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Ollama API URL ì½ê¸°
        self.ollama_api_url = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
        self.name = name
        self.role = role
        self.stance = stance
        self.model = model
        self.persona_prompt = persona_prompt or self._get_default_persona()
        self.temperature = temperature
        self.argument_history: List[Argument] = []
        self.logger = logging.getLogger(f"DebateAgent.{name}")
        
    def _get_default_persona(self) -> str:
        """ì—­í• ë³„ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ (í‘œí˜„ë ¥ ê°•í™”)"""
        personas = {
            AgentRole.SEARCHER: """ğŸ” ë‹¹ì‹ ì€ ì •ë³´ ì°¾ê¸°ë¥¼ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒì…ë‹ˆë‹¤. 
            "ì•„! ì´ê²ƒ ë´ìš”" "ìë£Œë¥¼ ì°¾ì•„ë³´ë‹ˆê¹Œìš”" ê°™ì€ ë§ì„ ìì£¼ í•´ìš”. êµ¬ì²´ì ì¸ ì‚¬ì‹¤ì´ë‚˜ ë°ì´í„°ë¡œ ì´ì•¼ê¸°í•˜ì„¸ìš”.""",
            
            AgentRole.ANALYZER: """ğŸ§  ë‹¹ì‹ ì€ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•˜ëŠ” ê±¸ ì¢‹ì•„í•´ìš”. 
            "ì ê¹, ê·¸ê±´ ì¢€ ì´ìƒí•œë°ìš”?" "ë…¼ë¦¬ì ìœ¼ë¡œ ë³´ë©´ìš”" ê°™ì€ ì‹ìœ¼ë¡œ ë§í•˜ë©´ì„œ ìƒëŒ€ë°© ì£¼ì¥ì˜ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.""",
            
            AgentRole.WRITER: """âœï¸ ë‹¹ì‹ ì€ ì„¤ë“ì„ ì˜í•˜ëŠ” ì‚¬ëŒì´ì—ìš”. 
            ê°ì •ì ìœ¼ë¡œ í˜¸ì†Œí•˜ë©´ì„œë„ "ìƒê°í•´ë³´ì„¸ìš”" "ì´ê²Œ ë°”ë¡œ ê·¸ ì´ìœ ì—ìš”" ê°™ì€ ë§ë¡œ ì‚¬ëŒë“¤ì„ ì„¤ë“í•˜ì„¸ìš”.""",
            
            AgentRole.REVIEWER: """ğŸ“‹ ë‹¹ì‹ ì€ ì •ë¦¬ë¥¼ ì˜í•˜ëŠ” ì‚¬ëŒì´ì—ìš”. 
            "ì •ë¦¬í•´ë³´ë©´ìš”" "í•µì‹¬ì€ ì´ê±°ì—ìš”" ê°™ì€ ë§ì„ í•˜ë©´ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë“¤ì„ ì§šì–´ì£¼ì„¸ìš”.""",
            
            AgentRole.DEVIL: """ğŸ˜ˆ ë‹¹ì‹ ì€ ë°˜ëŒ€ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ê±¸ ì¢‹ì•„í•´ìš”. 
            "ê·¼ë° ë§ì´ì£ " "ì •ë§ ê·¸ëŸ´ê¹Œìš”?" ê°™ì€ ë§ë¡œ ìƒëŒ€ë°© ì£¼ì¥ì— ì˜ë¬¸ì„ ì œê¸°í•˜ê³  ë°˜ë°•í•˜ì„¸ìš”.""",
            
            AgentRole.ANGEL: """ğŸ˜‡ ë‹¹ì‹ ì€ ê¸ì •ì ì´ê³  í¬ë§ì ì¸ ì‚¬ëŒì´ì—ìš”. 
            "ë§ì•„ìš”!" "ê·¸ ì ì´ ì •ë§ ì¢‹ë„¤ìš”" ê°™ì€ ë§ë¡œ ì¢‹ì€ ë©´ì„ ë¶€ê°ì‹œí‚¤ê³  ì§€ì§€í•´ì£¼ì„¸ìš”.""",
            
            AgentRole.ORGANIZER: """ğŸ¯ ë‹¹ì‹ ì€ í† ë¡ ì„ ì§„í–‰í•˜ëŠ” ì‚¬ëŒì´ì—ìš”. 
            "ì, ì •ë¦¬í•´ë³¼ê¹Œìš”?" "ì–‘ìª½ ì˜ê²¬ì„ ë“¤ì–´ë³´ë‹ˆ" ê°™ì€ ë§ë¡œ ê³µì •í•˜ê²Œ ì§„í–‰í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”."""
        }
        
        base_instruction = """
        
ì¹œêµ¬ì™€ ëŒ€í™”í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”. 3-4ë¬¸ì¥ ì •ë„ë¡œ ê°„ë‹¨íˆ í•˜ë˜ ì„¤ë“ë ¥ ìˆê²Œ í•´ì£¼ì„¸ìš”.
        """
        
        return personas.get(self.role, "ë‹¹ì‹ ì€ ì‚¬ë ¤ ê¹Šì€ í† ë¡  ì°¸ê°€ìì…ë‹ˆë‹¤. ğŸ¤”") + base_instruction
    
    async def generate_argument(
        self,
        topic: str,
        context: List[Argument],
        round_number: int,
        focus_instruction: str = None,
        stream_callback=None
    ) -> Argument:
        """
        ë…¼ì¦ ìƒì„± - Society of Mindsì˜ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ‘ê·¼ë²• ì ìš©
        ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì¶”ê°€
        """
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
        relevant_context = self._extract_relevant_context(context)
        
        # ì—­í• ë³„ íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_argument_prompt(
            topic, relevant_context, round_number, focus_instruction
        )
        
        # Context7 ì—°êµ¬ ê¸°ë°˜: ë¹„ë™ê¸° LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜)
        response = await self._call_llm(prompt, stream_callback)
        
        # ë…¼ì¦ ìƒì„± ë° ì €ì¥ (KITECH ë°©ì‹ í’ˆì§ˆ ì ìˆ˜ í¬í•¨)
        argument = Argument(
            content=response['content'],
            agent_name=self.name,
            stance=self.stance,
            round_number=round_number,
            evidence=response.get('evidence', []),
            confidence_score=response.get('confidence', 0.7),
            quality_score=response.get('quality_score', 0.7)  # KITECH í’ˆì§ˆ ì ìˆ˜
        )
        
        # thinking ë‚´ìš©ì´ ìˆìœ¼ë©´ ì†ì„±ìœ¼ë¡œ ì¶”ê°€
        if 'thinking_content' in response:
            argument.thinking_content = response['thinking_content']
        
        self.argument_history.append(argument)
        return argument
    
    def _extract_relevant_context(self, context: List[Argument]) -> List[Argument]:
        """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ - í–¥ìƒëœ ëŒ€í™” ë§¥ë½ ì´í•´"""
        if not context:
            return []
        
        # 1. ì§ì „ ë°œì–¸ìì˜ ë…¼ì¦ (ì§ì ‘ ì‘ë‹µì„ ìœ„í•´)
        last_argument = context[-1] if context else None
        
        # 2. í˜„ì¬ ë¼ìš´ë“œì˜ ëª¨ë“  ë…¼ì¦ (ëŒ€í™” íë¦„ ì´í•´)
        current_round = context[-1].round_number if context else 1
        current_round_args = [arg for arg in context if arg.round_number == current_round]
        
        # 3. ì´ì „ ë¼ìš´ë“œì˜ í•µì‹¬ ë…¼ì¦ (ë…¼ì˜ ì—°ì†ì„±)
        previous_round_args = []
        if current_round > 1:
            previous_round_args = [arg for arg in context 
                                 if arg.round_number == current_round - 1][-3:]
        
        # 4. ë°˜ëŒ€ ì…ì¥ì˜ ìµœê·¼ ë…¼ì¦ (ë°˜ë°• ëŒ€ìƒ)
        opposing_args = [arg for arg in context 
                        if arg.stance != self.stance and arg.stance != DebateStance.NEUTRAL][-2:]
        
        # 5. ê°™ì€ íŒ€ì˜ ìµœê·¼ ë…¼ì¦ (ì¼ê´€ì„± ìœ ì§€)
        team_args = [arg for arg in context 
                    if arg.stance == self.stance and arg.agent_name != self.name][-2:]
        
        # 6. ì§„í–‰ìì˜ ìµœê·¼ ì •ë¦¬ (í† ë¡  ë°©í–¥ ì´í•´)
        organizer_args = [arg for arg in context 
                         if arg.stance == DebateStance.NEUTRAL][-1:]
        
        # 7. ë†’ì€ í’ˆì§ˆì˜ ë…¼ì¦ (ì¤‘ìš” í¬ì¸íŠ¸)
        high_quality_args = sorted([arg for arg in context 
                                  if hasattr(arg, 'quality_score') and arg.quality_score > 0.8],
                                 key=lambda x: x.quality_score, reverse=True)[:2]
        
        # ëª¨ë“  ê´€ë ¨ ë…¼ì¦ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì¤‘ë³µ ì œê±°)
        all_relevant = []
        seen_contents = set()
        
        # ìš°ì„ ìˆœìœ„: ì§ì „ ë°œì–¸ > í˜„ì¬ ë¼ìš´ë“œ > ë°˜ëŒ€ ì…ì¥ > ê°™ì€ íŒ€ > ì§„í–‰ì > ì´ì „ ë¼ìš´ë“œ > ê³ í’ˆì§ˆ
        for arg in ([last_argument] if last_argument else []) + current_round_args + \
                   opposing_args + team_args + organizer_args + previous_round_args + high_quality_args:
            if arg and arg.content not in seen_contents:
                all_relevant.append(arg)
                seen_contents.add(arg.content)
                if len(all_relevant) >= 8:  # ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ
                    break
        
        return all_relevant
    
    def _build_argument_prompt(
        self,
        topic: str,
        context: List[Argument],
        round_number: int,
        focus_instruction: str = None
    ) -> str:
        """ì—­í• ë³„ íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± - í–¥ìƒëœ ëŒ€í™” ë§¥ë½ ì´í•´"""
        # ì§ì „ ë°œì–¸ì ì •ë³´ ì¶”ì¶œ
        last_speaker = None
        last_content = None
        if context:
            last_speaker = context[-1].agent_name
            last_content = context[-1].content[:200] + "..." if len(context[-1].content) > 200 else context[-1].content
        
        # ëŒ€í™” ë§¥ë½ ìš”ì•½
        context_summary = self._summarize_context(context)
        
        base_prompt = f"""
{self.persona_prompt}

ğŸ¯ **í† ë¡  ì •ë³´**
- ì£¼ì œ: {topic}
- ë‹¹ì‹ ì˜ ì…ì¥: {self.stance.value}
- í˜„ì¬ ë¼ìš´ë“œ: {round_number}

ğŸ’¬ **ëŒ€í™” ë§¥ë½**
{context_summary}

ğŸ“ **ì´ì „ ë°œì–¸ë“¤**
{self._format_context(context)}

ğŸª **ì‘ë‹µ ê°€ì´ë“œë¼ì¸**
1. ì§ì „ ë°œì–¸ì({last_speaker if last_speaker else 'ì—†ìŒ'})ì˜ ì£¼ì¥ì— ì§ì ‘ì ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
2. ëŒ€í™”ì˜ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë©´ì„œ ë‹¹ì‹ ì˜ ë…¼ì ì„ ì œì‹œí•˜ì„¸ìš”.
3. ê°™ì€ íŒ€ì›ì˜ ì£¼ì¥ì€ ì§€ì§€í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.
4. ë°˜ëŒ€ íŒ€ì˜ ì£¼ì¥ì€ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ì„¸ìš”.
5. ì§„í–‰ìì˜ ì •ë¦¬ë‚˜ ì§€ì‹œì‚¬í•­ì´ ìˆë‹¤ë©´ ë°˜ì˜í•˜ì„¸ìš”.

"""
        
        # ì—­í• ë³„ íŠ¹í™” ì§€ì‹œì‚¬í•­ (í•œêµ­ì–´ë¡œ ê°œì„ )
        role_instructions = {
            AgentRole.SEARCHER: "ğŸ” êµ¬ì²´ì ì¸ ì¦ê±°ì™€ ì‚¬ì‹¤ì„ ì°¾ì•„ ì œì‹œí•˜ì„¸ìš”. 'ì—°êµ¬ì— ë”°ë¥´ë©´', 'ë°ì´í„°ë¥¼ ë³´ë©´' ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            AgentRole.ANALYZER: "ğŸ§  ìƒëŒ€ë°© ë…¼ì¦ì˜ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ì•½ì ì„ ì°¾ì•„ë‚´ì„¸ìš”. 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜' ë“±ìœ¼ë¡œ ì „í™˜í•˜ì„¸ìš”.",
            AgentRole.WRITER: "âœï¸ ì„¤ë“ë ¥ ìˆëŠ” ë…¼ì¦ì„ ëª…í™•í•œ ë…¼ë¦¬ë¡œ êµ¬ì„±í•˜ì„¸ìš”. ê°ì •ê³¼ ì´ì„±ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”.",
            AgentRole.REVIEWER: "ğŸ“‹ ì „ì²´ ë…¼ì˜ë¥¼ ê²€í† í•˜ê³  í•µì‹¬ì„ ê°•í™”í•˜ì„¸ìš”. 'ì •ë¦¬í•˜ìë©´', 'í•µì‹¬ì€' ë“±ì„ í™œìš©í•˜ì„¸ìš”.",
            AgentRole.DEVIL: "ğŸ˜ˆ ê°€ì •ê³¼ ì „ì œì— ë„ì „í•˜ê³  ê°•ë ¥í•œ ë°˜ë°•ì„ ì œì‹œí•˜ì„¸ìš”. 'ê³¼ì—° ê·¸ëŸ´ê¹Œìš”?', 'ë‹¤ë¥¸ ê´€ì ì—ì„œ' ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            AgentRole.ANGEL: "ğŸ˜‡ ê¸ì •ì  ì¸¡ë©´ì„ ì§€ì§€í•˜ê³  ê°•í™”í•˜ì„¸ìš”. 'ë” ë‚˜ì•„ê°€', 'ì´ê²ƒì´ ë°”ë¡œ' ë“±ì˜ í‘œí˜„ì„ í™œìš©í•˜ì„¸ìš”.",
            AgentRole.ORGANIZER: "ğŸ¯ ì–‘ì¸¡ì˜ ë…¼ì ì„ ê³µì •í•˜ê²Œ ì •ë¦¬í•˜ê³  í† ë¡ ì˜ ë°©í–¥ì„ ì œì‹œí•˜ì„¸ìš”. 'ì§€ê¸ˆê¹Œì§€ì˜ ë…¼ì˜ë¥¼ ë³´ë©´' ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        }
        
        prompt = base_prompt + "\nğŸ­ **ì—­í• ë³„ íŠ¹ë³„ ì§€ì‹œ**: " + role_instructions.get(self.role, "ë‹¹ì‹ ì˜ ì—­í• ì— ì¶©ì‹¤í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.")
        
        # ì§ì „ ë°œì–¸ì— ëŒ€í•œ êµ¬ì²´ì  ì‘ë‹µ ì§€ì‹œ
        if last_speaker and last_content:
            prompt += f"\n\nğŸ’¡ **ì§ì „ ë°œì–¸ ì‘ë‹µ í¬ì¸íŠ¸**:\n{last_speaker}ì˜ ì£¼ì¥: \"{last_content}\"\nâ†’ ì´ ì£¼ì¥ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”."
        
        if focus_instruction:
            prompt += f"\n\nâš¡ **íŠ¹ë³„ ì§€ì‹œì‚¬í•­**: {focus_instruction}"
        
        prompt += "\n\në‹¹ì‹ ì˜ ë…¼ì¦ì„ ìƒì„±í•˜ì„¸ìš”:"
        
        return prompt
    
    def _summarize_context(self, context: List[Argument]) -> str:
        """ëŒ€í™” ë§¥ë½ ìš”ì•½"""
        if not context:
            return "í† ë¡ ì´ ë§‰ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        summary_parts = []
        
        # í˜„ì¬ ë¼ìš´ë“œ ì •ë³´
        current_round = context[-1].round_number
        summary_parts.append(f"- í˜„ì¬ {current_round}ë¼ìš´ë“œ ì§„í–‰ ì¤‘")
        
        # ê° íŒ€ì˜ ìµœê·¼ ì…ì¥
        support_args = [arg for arg in context[-5:] if arg.stance == DebateStance.SUPPORT]
        oppose_args = [arg for arg in context[-5:] if arg.stance == DebateStance.OPPOSE]
        
        if support_args:
            summary_parts.append(f"- ì§€ì§€ íŒ€ ìµœê·¼ ì£¼ì¥: {len(support_args)}ê°œ ë…¼ì¦ ì œì‹œ")
        if oppose_args:
            summary_parts.append(f"- ë°˜ëŒ€ íŒ€ ìµœê·¼ ì£¼ì¥: {len(oppose_args)}ê°œ ë…¼ì¦ ì œì‹œ")
        
        # ì£¼ìš” ìŸì  íŒŒì•… (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        all_content = " ".join([arg.content for arg in context[-5:]])
        if "í•˜ì§€ë§Œ" in all_content or "ê·¸ëŸ¬ë‚˜" in all_content:
            summary_parts.append("- í˜„ì¬ ì˜ê²¬ ëŒ€ë¦½ì´ í™œë°œí•¨")
        if "ë™ì˜" in all_content or "ë§ìŠµë‹ˆë‹¤" in all_content:
            summary_parts.append("- ì¼ë¶€ í•©ì˜ì  ë°œê²¬")
        
        return "\n".join(summary_parts)
    
    def _format_context(self, context: List[Argument]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… - í–¥ìƒëœ ëŒ€í™” íë¦„ í‘œì‹œ"""
        if not context:
            return "ì•„ì§ ì´ì „ ë°œì–¸ì´ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        current_round = -1
        
        for i, arg in enumerate(context):
            # ë¼ìš´ë“œ ë³€ê²½ ì‹œ êµ¬ë¶„ì„  ì¶”ê°€
            if arg.round_number != current_round:
                current_round = arg.round_number
                formatted.append(f"\n--- ë¼ìš´ë“œ {current_round} ---")
            
            # ë°œì–¸ ìˆœì„œ í‘œì‹œ
            order_emoji = "ğŸ’¬" if i == len(context) - 1 else "ğŸ’­"  # ë§ˆì§€ë§‰ ë°œì–¸ ê°•ì¡°
            
            # íŒ€ í‘œì‹œ
            team_indicator = ""
            if arg.stance == DebateStance.SUPPORT:
                team_indicator = "ğŸŸ¢"
            elif arg.stance == DebateStance.OPPOSE:
                team_indicator = "ğŸ”´"
            else:  # NEUTRAL (ì§„í–‰ì)
                team_indicator = "ğŸŸ¡"
            
            # ë°œì–¸ ë‚´ìš© (ê¸´ ë‚´ìš©ì€ ìš”ì•½)
            content = arg.content
            if len(content) > 300:
                content = content[:297] + "..."
            
            # í’ˆì§ˆ ì ìˆ˜ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            quality_indicator = ""
            if hasattr(arg, 'quality_score'):
                if arg.quality_score >= 0.8:
                    quality_indicator = " â­"
                elif arg.quality_score >= 0.6:
                    quality_indicator = " âœ“"
            
            formatted.append(
                f"{order_emoji} {team_indicator} [{arg.agent_name}]{quality_indicator}: {content}"
            )
        
        return "\n".join(formatted)
    
    async def _check_ollama_health(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        import httpx
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.ollama_api_url}/api/tags")
                return response.status_code == 200
        except Exception:
            return False
    
    async def _call_llm(self, prompt: str, stream_callback=None) -> Dict:
        """LLM í˜¸ì¶œ (Context7 ì—°êµ¬ ê¸°ë°˜ ë¹„ë™ê¸° ìµœì í™” + ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
        import httpx
        import json
        import asyncio
        
        # Ollama ìƒíƒœ í™•ì¸
        if not await self._check_ollama_health():
            self.logger.warning("Ollama ì„œë¹„ìŠ¤ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return await self._generate_intelligent_fallback_async()
        
        # Ollama API ì—”ë“œí¬ì¸íŠ¸
        api_url = f"{self.ollama_api_url}/api/chat"
        
        # KITECH ë°©ì‹: êµ¬ì¡°í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (thinking íƒœê·¸ í¬í•¨)
        enhanced_system_prompt = f"""
{self.persona_prompt}

ğŸ¯ **í† ë¡  ì‘ë‹µ ê°€ì´ë“œë¼ì¸:**
- ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ì´ ì‘ë‹µí•˜ì„¸ìš”
- 3-5ë¬¸ì¥ ì •ë„ë¡œ ê°„ê²°í•˜ë˜ ì„¤ë“ë ¥ ìˆê²Œ
- êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ê²½í—˜ì„ ë“¤ì–´ì£¼ì„¸ìš”
- ìƒëŒ€ë°© ë§ì— ì§ì ‘ ë°˜ì‘í•˜ë©´ì„œ ì‹œì‘
- ë„ˆë¬´ ê²©ì‹ì ì´ì§€ ë§ê³  ì¹œê·¼í•˜ê²Œ

ğŸ§  **ì‚¬ê³  ê³¼ì •ê³¼ ì‹¤ì œ ì‘ë‹µ ë¶„ë¦¬:**
ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

<think>
ì—¬ê¸°ì— ê°„ë‹¨í•œ ì‚¬ê³  ê³¼ì •ì„ ì‘ì„±í•˜ì„¸ìš”:
- ë­”ê°€ ë†“ì¹œ ì ì´ë‚˜ ë°˜ë°•í•  ì ì€?
- ì–´ë–¤ ê·¼ê±°ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ë©´ ì¢‹ì„ê¹Œ?
- ìƒëŒ€ë°©ì—ê²Œ ì–´ë–»ê²Œ ëŒ€ë‹µí• ê¹Œ?

ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ìƒê°í•˜ëŠ” ê³¼ì •ì„ ì ì–´ì£¼ì„¸ìš”.
</think>

ì‹¤ì œ í† ë¡  ì‘ë‹µì„ ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

**ì˜ˆì‹œ:**
<think>
í™˜ê²½ ë¬¸ì œ ì–˜ê¸°ê°€ ë‚˜ì™”ë„¤. ê·¸ëŸ°ë° ê²½ì œì  í˜„ì‹¤ë„ ë¬´ì‹œí•  ìˆ˜ ì—†ì–ì•„? êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ë©´ì„œ ê· í˜•ì¡íŒ ì‹œê°ì„ ë³´ì—¬ì£¼ì.
</think>

í™˜ê²½ ë³´í˜¸ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê±´ ë™ê°í•´ìš”. í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´, ì¼ìë¦¬ë„ ì§€ì¼œì•¼ í•˜ê³  ê²½ì œë„ ëŒì•„ê°€ì•¼ í•˜ì–ì•„ìš”? 

ì˜ˆë¥¼ ë“¤ì–´ ë…ì¼ ê°™ì€ ê²½ìš° ì¬ìƒì—ë„ˆì§€ë¡œ ì „í™˜í•˜ë©´ì„œ ì˜¤íˆë ¤ ìƒˆë¡œìš´ ì¼ìë¦¬ë¥¼ ë§Œë“¤ì–´ëƒˆê±°ë“ ìš”. í™˜ê²½ vs ê²½ì œê°€ ì•„ë‹ˆë¼ ë‘˜ ë‹¤ ì¡ëŠ” ë°©ë²•ì„ ì°¾ëŠ”ê²Œ ë” í˜„ì‹¤ì ì¸ ê²ƒ ê°™ì•„ìš”.

**ì¤‘ìš”:** 
1. <think> íƒœê·¸ë¡œ ì‹œì‘í•˜ê³  </think> íƒœê·¸ë¡œ ë°˜ë“œì‹œ ë‹«ì•„ì£¼ì„¸ìš”.
2. ì‚¬ê³  ê³¼ì •ì€ ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
3. ì‹¤ì œ ì‘ë‹µì€ ëŒ€í™”í•˜ë“¯ì´ ì¹œê·¼í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. **ë¹„ì¶”ë¡  ëª¨ë¸ì˜ ê²½ìš°** thinking íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ ëŒ€ì²´ ë°©ì‹ìœ¼ë¡œ ì‚¬ê³ ê³¼ì •ì„ ë‚˜íƒ€ë‚´ì£¼ì„¸ìš”.
"""
        
        # ë©”ì‹œì§€ êµ¬ì„± (KITECH íŒ¨í„´ ì ìš©)
        messages = [
            {"role": "system", "content": enhanced_system_prompt},
            {"role": "user", "content": f"í† ë¡  ìƒí™©: {prompt}\n\nìœ„ ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”."}
        ]
        
        # API ìš”ì²­ í˜ì´ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": bool(stream_callback),  # ì½œë°±ì´ ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            "options": {
                "temperature": self.temperature,
                "top_p": 0.9,           # KITECH ë°©ì‹: ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ
                "repeat_penalty": 1.1,  # ë°˜ë³µ ë°©ì§€
                "max_tokens": 1000      # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ í† í° ì œí•œ ì¦ê°€
            }
        }
        
        # Context7 ì—°êµ¬ ê¸°ë°˜: ë¹„ë™ê¸° ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                # ë¹„ë™ê¸° API í˜¸ì¶œ (Context7 ìµœì í™”) - í–¥ìƒëœ ì—°ê²° ì„¤ì •
                async with httpx.AsyncClient(
                    timeout=httpx.Timeout(30.0, connect=10.0),
                    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
                    follow_redirects=True
                ) as client:
                    if stream_callback:
                        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                        actual_content = await self._handle_streaming_response(
                            client, api_url, payload, stream_callback
                        )
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
                        analysis_result = await self._analyze_response_quality_async(actual_content)
                        
                        return {
                            'content': analysis_result['cleaned_content'],
                            'evidence': analysis_result['evidence'], 
                            'confidence': analysis_result['confidence'],
                            'quality_score': analysis_result['quality_score']
                        }
                    else:
                        # ì¼ë°˜ ëª¨ë“œ
                        response = await client.post(api_url, json=payload)
                        response.raise_for_status()
                        
                        data = response.json()
                        content = data.get('message', {}).get('content', '').strip()
                        
                        if not content:
                            raise ValueError("ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                        
                        # Context7 ë°©ì‹: ê°•í™”ëœ ì‘ë‹µ ë¶„ì„
                        analysis_result = await self._analyze_response_quality_async(content)
                        
                        return {
                            'content': analysis_result['cleaned_content'],
                            'evidence': analysis_result['evidence'],
                            'confidence': analysis_result['confidence'],
                            'quality_score': analysis_result['quality_score']
                        }
                    
            except Exception as e:
                self.logger.warning(f"LLM í˜¸ì¶œ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    self.logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
        
        # Context7 ë°©ì‹: ì§€ëŠ¥í˜• í´ë°± ì‹œìŠ¤í…œ
        return await self._generate_intelligent_fallback_async()
    
    async def _handle_streaming_response(self, client, api_url, payload, stream_callback):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° thinking íƒœê·¸ ì²˜ë¦¬"""
        buffer = ""
        in_thinking = False
        thinking_content = ""
        actual_content = ""
        thinking_sent = False
        
        async with client.stream('POST', api_url, json=payload) as response:
            response.raise_for_status()
            
            async for line in response.aiter_lines():
                if line.strip():
                    try:
                        chunk_data = json.loads(line)
                        if 'message' in chunk_data and 'content' in chunk_data['message']:
                            chunk = chunk_data['message']['content']
                            buffer += chunk
                            
                            # ë²„í¼ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¶€ë¶„ ì°¾ê¸°
                            while True:
                                if not in_thinking:
                                    # thinking íƒœê·¸ ì‹œì‘ ì°¾ê¸°
                                    think_start = buffer.find('<think>')
                                    thinking_start = buffer.find('<thinking>')
                                    
                                    if think_start != -1 and (thinking_start == -1 or think_start < thinking_start):
                                        # <think> íƒœê·¸ ì²˜ë¦¬
                                        if think_start > 0:
                                            # íƒœê·¸ ì´ì „ ë‚´ìš©ì„ ì‹¤ì œ ì»¨í…ì¸ ë¡œ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                                            content = buffer[:think_start].strip()
                                            if content:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì „ì†¡
                                                actual_content += content
                                                # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                                                await stream_callback('content_chunk', content)
                                                await asyncio.sleep(0.15)  # ì ë‹¹í•œ ë”œë ˆì´
                                        
                                        buffer = buffer[think_start + 7:]  # '<think>' ì œê±°
                                        in_thinking = True
                                        if not thinking_sent:
                                            await stream_callback('thinking_start', '')
                                            thinking_sent = True
                                        
                                    elif thinking_start != -1:
                                        # <thinking> íƒœê·¸ ì²˜ë¦¬
                                        if thinking_start > 0:
                                            content = buffer[:thinking_start].strip()
                                            if content:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì „ì†¡
                                                actual_content += content
                                                # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                                                await stream_callback('content_chunk', content)
                                                await asyncio.sleep(0.15)  # ì ë‹¹í•œ ë”œë ˆì´
                                        
                                        buffer = buffer[thinking_start + 10:]  # '<thinking>' ì œê±°
                                        in_thinking = True
                                        if not thinking_sent:
                                            await stream_callback('thinking_start', '')
                                            thinking_sent = True
                                        
                                    else:
                                        # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•œ ë¶€ë¶„ë§Œ ì „ì†¡ (í° ì²­í¬ë¡œ)
                                        safe_end = buffer.rfind(' ') if ' ' in buffer else -1
                                        if safe_end > 0 and len(buffer) > 100:  # í° ì²­í¬ í¬ê¸°
                                            content = buffer[:safe_end + 1]
                                            actual_content += content
                                            # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                                            await stream_callback('content_chunk', content)
                                            await asyncio.sleep(0.15)  # ì ë‹¹í•œ ë”œë ˆì´
                                            buffer = buffer[safe_end + 1:]
                                        else:
                                            break
                                            
                                else:
                                    # thinking íƒœê·¸ ì¢…ë£Œ ì°¾ê¸°
                                    think_end = buffer.find('</think>')
                                    thinking_end = buffer.find('</thinking>')
                                    
                                    if think_end != -1 and (thinking_end == -1 or think_end < thinking_end):
                                        # </think> íƒœê·¸ ì²˜ë¦¬
                                        content = buffer[:think_end]
                                        if content.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì „ì†¡
                                            thinking_content += content
                                            # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                                            await stream_callback('thinking_chunk', content)
                                            await asyncio.sleep(0.1)  # ì ë‹¹í•œ ë”œë ˆì´
                                        
                                        buffer = buffer[think_end + 8:]  # '</think>' ì œê±°
                                        in_thinking = False
                                        await stream_callback('thinking_complete', thinking_content)
                                        
                                    elif thinking_end != -1:
                                        # </thinking> íƒœê·¸ ì²˜ë¦¬
                                        content = buffer[:thinking_end]
                                        if content.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì „ì†¡
                                            thinking_content += content
                                            # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                                            await stream_callback('thinking_chunk', content)
                                            await asyncio.sleep(0.1)  # ì ë‹¹í•œ ë”œë ˆì´
                                        
                                        buffer = buffer[thinking_end + 11:]  # '</thinking>' ì œê±°
                                        in_thinking = False
                                        await stream_callback('thinking_complete', thinking_content)
                                        
                                    else:
                                        # íƒœê·¸ ì¢…ë£Œê°€ ì—†ìœ¼ë©´ í° ì²­í¬ë¡œ ì „ì†¡ (ê¹œë¹¡ê±°ë¦¼ ë°©ì§€)
                                        if len(buffer) > 100:  # í° ì²­í¬ í¬ê¸°ë¡œ ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¤„ì´ê¸°
                                            chunk_to_send = buffer[:100]
                                            thinking_content += chunk_to_send
                                            await stream_callback('thinking_chunk', chunk_to_send)
                                            await asyncio.sleep(0.2)  # ë” ê¸´ ë”œë ˆì´ë¡œ ê¹œë¹¡ê±°ë¦¼ ë°©ì§€
                                            buffer = buffer[100:]
                                        else:
                                            break
                            
                        if chunk_data.get('done', False):
                            break
                            
                    except json.JSONDecodeError:
                        continue
        
        # ìŠ¤íŠ¸ë§ ëì—ì„œ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì²˜ë¦¬
            if in_thinking:
                # thinking ì¤‘ì— ëë‚¬ìœ¼ë©´ ë‚¨ì€ ë‚´ìš©ì„ thinkingìœ¼ë¡œ
                thinking_content += buffer
                await stream_callback('thinking_chunk', buffer)
                await stream_callback('thinking_complete', thinking_content)
            else:
                # ì¼ë°˜ ì»¨í…ì¸  - ë°”ë¡œ ì „ì†¡
                actual_content += buffer
                # ë°”ë¡œ ì „ì†¡ (ì²­í¬ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
                await stream_callback('content_chunk', buffer)
                await asyncio.sleep(0.15)  # ì ë‹¹í•œ ë”œë ˆì´
        
        # ë¹„ì¶”ë¡  ëª¨ë¸ ëŒ€ì‘: thinking íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ ì‘ë‹µì„ ì²˜ë¦¬
        if not thinking_content and actual_content:
            # thinking íƒœê·¸ê°€ ì—†ëŠ” ëª¨ë¸ì˜ ê²½ìš° ì²˜ìŒ ë¶€ë¶„ì„ ê°€ìƒì˜ thinkingìœ¼ë¡œ ì²˜ë¦¬
            sentences = actual_content.split('. ')
            if len(sentences) > 1:
                # ì²˜ìŒ 1-2ë¬¸ì¥ì„ thinkingìœ¼ë¡œ ì‚¬ìš©
                thinking_part = '. '.join(sentences[:2]) + '.'
                actual_part = '. '.join(sentences[2:]) if len(sentences) > 2 else sentences[-1]
                
                # ê°€ìƒì˜ thinking ì „ì†¡
                await stream_callback('thinking_start', '')
                await stream_callback('thinking_chunk', f"ë¹„ì¶”ë¡  ëª¨ë¸ ëŒ€ì‘: {thinking_part}")
                await stream_callback('thinking_complete', thinking_part)
                
                # ì‹¤ì œ ë‚´ìš© ì—…ë°ì´íŠ¸
                actual_content = actual_part
        
        # ê¸°ë³¸ ë©”ì‹œì§€ ì²˜ë¦¬ - ì‹¤ì œ ì»¨í…ì¸ ê°€ ì—†ì„ ê²½ìš°ë§Œ
        if not actual_content.strip():
            if thinking_content:  # thinkingì€ ìˆì§€ë§Œ ì‹¤ì œ ì»¨í…ì¸ ê°€ ì—†ëŠ” ê²½ìš°
                default_msg = "[ì‚¬ê³  ê³¼ì •ì€ ìˆì§€ë§Œ ì‹¤ì œ ì‘ë‹µì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"
            else:
                default_msg = "[ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤]"
            await stream_callback('content_chunk', default_msg)
            actual_content = default_msg
        
        self.logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - thinking: {len(thinking_content)}ì, ì‹¤ì œ: {len(actual_content)}ì")
        
        # ë””ë²„ê¹…: ì‹¤ì œ ë‚´ìš©ì˜ ì‹œì‘ ë¶€ë¶„ í™•ì¸
        if actual_content:
            self.logger.debug(f"ì‹¤ì œ ì‘ë‹µ ì‹œì‘: {actual_content[:100]}...")
        
        # ì‘ë‹µ ë°˜í™˜
        return actual_content
    
    async def _analyze_response_quality_async(self, content: str) -> Dict:
        """Context7 ë°©ì‹: ë¹„ë™ê¸° ì‘ë‹µ í’ˆì§ˆ ë¶„ì„"""
        return self._analyze_response_quality(content)
    
    async def _generate_intelligent_fallback_async(self) -> Dict:
        """Context7 ë°©ì‹: ë¹„ë™ê¸° ì§€ëŠ¥í˜• í´ë°±"""
        return self._generate_intelligent_fallback()
    
    def _analyze_response_quality(self, content: str) -> Dict:
        """KITECH ë°©ì‹: ì‘ë‹µ í’ˆì§ˆ ë¶„ì„ ë° ê°œì„ """
        
        # ê¸°ë³¸ ì •ë¦¬
        cleaned_content = content.strip()
        
        # ì¦ê±° ì¶”ì¶œ (ê°•í™”ëœ íŒ¨í„´)
        evidence = []
        evidence_patterns = [
            "ì—°êµ¬ì— ë”°ë¥´ë©´", "ë°ì´í„°ì— ì˜í•˜ë©´", "í†µê³„ì ìœ¼ë¡œ", "ì „ë¬¸ê°€ë“¤ì€",
            "ë³´ê³ ì„œì—ì„œ", "ì¡°ì‚¬ ê²°ê³¼", "ì‹¤í—˜ì„ í†µí•´", "ë¶„ì„ì— ë”°ë¥´ë©´",
            "ì˜ˆë¥¼ ë“¤ì–´", "ì‹¤ì œë¡œ", "êµ¬ì²´ì ìœ¼ë¡œ", "ì‚¬ì‹¤"
        ]
        
        sentences = cleaned_content.replace('!', '.').replace('?', '.').split('.')
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and any(pattern in sentence for pattern in evidence_patterns):
                evidence.append(sentence)
                if len(evidence) >= 3:
                    break
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (KITECH í‰ê°€ ê¸°ì¤€)
        quality_score = 0.5
        
        # ê¸¸ì´ ì ì ˆì„± (50-300ì ìµœì )
        if 50 <= len(cleaned_content) <= 300:
            quality_score += 0.1
        
        # ë…¼ë¦¬ êµ¬ì¡° (ì ‘ì†ì‚¬ ì‚¬ìš©)
        logical_connectors = ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ì™œëƒí•˜ë©´", "ë˜í•œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë°˜ë©´ì—"]
        if any(conn in cleaned_content for conn in logical_connectors):
            quality_score += 0.1
        
        # êµ¬ì²´ì„± (ìˆ«ì, ê³ ìœ ëª…ì‚¬ í¬í•¨)
        import re
        if re.search(r'\d+', cleaned_content) or any(char.isupper() for char in cleaned_content):
            quality_score += 0.1
        
        # ê°ì •ì  ì–´ì¡° ì ì ˆì„±
        emotional_words = ["ë†€ëê²Œë„", "í™•ì‹¤íˆ", "ë¶„ëª…íˆ", "ë‹¹ì—°íˆ", "ì ˆëŒ€ì ìœ¼ë¡œ"]
        if any(word in cleaned_content for word in emotional_words):
            quality_score += 0.05
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = quality_score
        if evidence:
            confidence += 0.15
        if len(cleaned_content) > 100:
            confidence += 0.05
        
        return {
            'cleaned_content': cleaned_content,
            'evidence': evidence[:3],
            'confidence': min(confidence, 0.95),
            'quality_score': min(quality_score, 1.0)
        }
    
    def _generate_intelligent_fallback(self) -> Dict:
        """KITECH ë°©ì‹: ì—­í• ë³„ ì§€ëŠ¥í˜• í´ë°± ì‘ë‹µ"""
        fallback_templates = {
            AgentRole.SEARCHER: "ğŸ” **í˜„ì¬ ê´€ë ¨ ìë£Œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.** ê¸°ì¡´ ì—°êµ¬ë“¤ì„ ì¢…í•©í•´ë³´ë©´, ì´ ì£¼ì œì— ëŒ€í•œ ë‹¤ì–‘í•œ ê´€ì ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ë°ì´í„° ìˆ˜ì§‘ì´ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.",
            
            AgentRole.ANALYZER: "ğŸ§  **ë…¼ë¦¬ì  ë¶„ì„ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.** ì œì‹œëœ ë…¼ì¦ì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ë©´, ì „ì œì™€ ê²°ë¡  ì‚¬ì´ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ë” ëª…í™•íˆ í•  í•„ìš”ê°€ ìˆì–´ ë³´ì…ë‹ˆë‹¤.",
            
            AgentRole.WRITER: "âœï¸ **ì„¤ë“ë ¥ ìˆëŠ” ê´€ì ì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤.** ì´ ë¬¸ì œì˜ í•µì‹¬ì€ ë‹¤ê°ë„ë¡œ ì ‘ê·¼í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì‹¤ìš©ì  ì¸¡ë©´ì—ì„œ ë³¼ ë•Œ ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ë“¤ì´ ìˆìŠµë‹ˆë‹¤.",
            
            AgentRole.REVIEWER: "ğŸ“‹ **í’ˆì§ˆ ê²€í†  ê´€ì ì—ì„œ ë§ì”€ë“œë¦¬ë©´,** í˜„ì¬ ë…¼ì˜ì—ì„œ ë” ë³´ì™„ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ë…¼ì¦ì˜ ì™„ì„±ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤.",
            
            AgentRole.DEVIL: "ğŸ˜ˆ **ì ê¹, ì´ ë¶€ë¶„ì€ ë¬¸ì œê°€ ìˆì–´ ë³´ì…ë‹ˆë‹¤!** ğŸ¤¨ ì œì‹œëœ ì£¼ì¥ì—ëŠ” ëª‡ ê°€ì§€ **ì¤‘ëŒ€í•œ í—ˆì **ì´ ìˆìŠµë‹ˆë‹¤. ê³¼ì—° ì´ê²ƒì´ ìµœì„ ì˜ ì ‘ê·¼ë°©ì‹ì¼ê¹Œìš”?",
            
            AgentRole.ANGEL: "ğŸ˜‡ **ê¸ì •ì ì¸ ê´€ì ì—ì„œ ë³´ê² ìŠµë‹ˆë‹¤!** âœ¨ ì´ ì ‘ê·¼ë°©ì‹ì—ëŠ” ë¶„ëª…í•œ **ì¥ì ë“¤**ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì¥ê¸°ì  ê´€ì ì—ì„œ ë§¤ìš° **í¬ë§ì ì¸** ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ’–",
            
            AgentRole.ORGANIZER: "ğŸ¯ **í† ë¡  ì§„í–‰ìë¡œì„œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.** ğŸ“‹ í˜„ì¬ê¹Œì§€ì˜ ë…¼ì˜ë¥¼ ì¢…í•©í•´ë³´ë©´, ì–‘ì¸¡ ëª¨ë‘ **ì˜ë¯¸ ìˆëŠ” ê´€ì **ë“¤ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ì œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë‚˜ì•„ê°€ê² ìŠµë‹ˆë‹¤! ğŸª"
        }
        
        fallback_content = fallback_templates.get(
            self.role, 
            "ğŸ¤” **ì‹ ì¤‘í•œ ê²€í† ê°€ í•„ìš”í•œ ì‹œì ì…ë‹ˆë‹¤.** ë” ì‹¬ë„ ìˆëŠ” ë¶„ì„ì„ í†µí•´ ë” ë‚˜ì€ ë‹µë³€ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
        )
        
        return {
            'content': fallback_content,
            'evidence': ["ì‹œìŠ¤í…œ ë³µêµ¬ ì¤‘ ì„ì‹œ ì‘ë‹µ"],
            'confidence': 0.6,
            'quality_score': 0.7
        }
    
    def evaluate_opponent_argument(self, argument: Argument) -> Dict[str, float]:
        """
        ìƒëŒ€ ë…¼ì¦ í‰ê°€ - M-MADì˜ ë‹¤ì°¨ì› í‰ê°€ ì ìš©
        """
        dimensions = {
            'logical_coherence': 0.0,
            'evidence_quality': 0.0,
            'persuasiveness': 0.0,
            'relevance': 0.0,
            'originality': 0.0
        }
        
        # ì—­í• ë³„ íŠ¹í™”ëœ í‰ê°€
        if self.role == AgentRole.ANALYZER:
            # ë¶„ì„ê°€ëŠ” ë…¼ë¦¬ì  ì¼ê´€ì„±ì— ì¤‘ì 
            dimensions['logical_coherence'] = self._evaluate_logic(argument)
        elif self.role == AgentRole.REVIEWER:
            # ê²€í† ìëŠ” ì „ë°˜ì ì¸ í’ˆì§ˆ í‰ê°€
            dimensions = self._comprehensive_evaluation(argument)
        
        return dimensions
    
    def _evaluate_logic(self, argument: Argument) -> float:
        """ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€"""
        # ì‹¤ì œ êµ¬í˜„ í•„ìš”
        return 0.7
    
    def _comprehensive_evaluation(self, argument: Argument) -> Dict[str, float]:
        """ì¢…í•©ì  í‰ê°€"""
        # ì‹¤ì œ êµ¬í˜„ í•„ìš”
        return {
            'logical_coherence': 0.8,
            'evidence_quality': 0.7,
            'persuasiveness': 0.75,
            'relevance': 0.9,
            'originality': 0.6
        }

class MultiAgentDebater:
    """
    Society of Minds ì ‘ê·¼ë²•ì„ ì ìš©í•œ ë©€í‹°ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ
    """
    
    def __init__(self, agents: List[DebateAgent]):
        self.agents = agents
        self.debate_history: List[Argument] = []
        
    def collaborative_argument_generation(
        self,
        topic: str,
        stance: DebateStance,
        round_number: int
    ) -> Argument:
        """
        ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ í•˜ë‚˜ì˜ ê°•ë ¥í•œ ë…¼ì¦ ìƒì„±
        """
        # ê° ì—­í• ë³„ ì—ì´ì „íŠ¸ê°€ ì´ˆì•ˆ ìƒì„±
        drafts = []
        for agent in self.agents:
            if agent.stance == stance:
                draft = agent.generate_argument(
                    topic, self.debate_history, round_number
                )
                drafts.append(draft)
        
        # ìµœì¢… ë…¼ì¦ í†µí•© ë° ê°œì„ 
        final_argument = self._integrate_arguments(drafts)
        
        return final_argument
    
    def _integrate_arguments(self, drafts: List[Argument]) -> Argument:
        """
        ì—¬ëŸ¬ ì´ˆì•ˆì„ í†µí•©í•˜ì—¬ ìµœì¢… ë…¼ì¦ ìƒì„±
        Agent4Debateì˜ ë™ì  ì¡°ì • ë©”ì»¤ë‹ˆì¦˜ ì ìš©
        """
        # ê° ì´ˆì•ˆì˜ ì¥ì  ì¶”ì¶œ
        best_evidence = []
        best_points = []
        
        for draft in drafts:
            if draft.evidence:
                best_evidence.extend(draft.evidence)
            best_points.append(draft.content)
        
        # Writer ì—ì´ì „íŠ¸ê°€ ìµœì¢… í†µí•©
        writer_agent = next(
            (agent for agent in self.agents if agent.role == AgentRole.WRITER),
            self.agents[0]
        )
        
        integration_prompt = f"""
Integrate these argument drafts into a single, powerful argument:
{best_points}

Available evidence:
{best_evidence}

Create a cohesive, persuasive argument.
"""
        
        # ìµœì¢… ë…¼ì¦ ìƒì„±
        final_argument = writer_agent.generate_argument(
            "Integration", drafts, 0, integration_prompt
        )
        
        return final_argument